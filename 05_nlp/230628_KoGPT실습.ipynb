{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9dd77294a36147a0b1ab8962089de8d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39daaa9b86b84679ba5241af58c842eb","IPY_MODEL_2df640a766034476aab9dc1a42ae8c62","IPY_MODEL_ac7b1cc8ac244f4faafb41044f35bb06"],"layout":"IPY_MODEL_0933782cc86f471e8ef55984f1b919de"}},"39daaa9b86b84679ba5241af58c842eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3c62caa311c4cdfadb43af95fedc720","placeholder":"​","style":"IPY_MODEL_709527935c8c461fa3f3e1161c1a3241","value":"Downloading (…)/main/tokenizer.json: 100%"}},"2df640a766034476aab9dc1a42ae8c62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0112c36948ad413bbcb9d57240db2b9f","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97719df472964939b1944d5ee22d3817","value":2825034}},"ac7b1cc8ac244f4faafb41044f35bb06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2d9c865b2854e32927bd5765eac9f82","placeholder":"​","style":"IPY_MODEL_28ca4995e46040aab3cb085f0472b5b5","value":" 2.83M/2.83M [00:00&lt;00:00, 4.56MB/s]"}},"0933782cc86f471e8ef55984f1b919de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3c62caa311c4cdfadb43af95fedc720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709527935c8c461fa3f3e1161c1a3241":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0112c36948ad413bbcb9d57240db2b9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97719df472964939b1944d5ee22d3817":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2d9c865b2854e32927bd5765eac9f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ca4995e46040aab3cb085f0472b5b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"674e2623d1564f09a92c65f38c9a2ed2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26b0e329808c482ba7008b847467f894","IPY_MODEL_960093b1dbb644b89dc12c0648946a17","IPY_MODEL_688d4209618347058df788b68227e114"],"layout":"IPY_MODEL_4e3f5ec6e2cd4105868bd34d729d0aa4"}},"26b0e329808c482ba7008b847467f894":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbd712642f00447085d396fceaf1f3f5","placeholder":"​","style":"IPY_MODEL_5e10d3367bdd441a832509299e59438d","value":"Downloading (…)lve/main/config.json: 100%"}},"960093b1dbb644b89dc12c0648946a17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37f137e40f864e73a79a1851e758c8d2","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_665ee1e2d66a47a4a9128484f11d4280","value":1000}},"688d4209618347058df788b68227e114":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d42dc9d2f994146a3b9cd0bc3182a72","placeholder":"​","style":"IPY_MODEL_586f1df46c8a40069afaf3395c2688ea","value":" 1.00k/1.00k [00:00&lt;00:00, 78.1kB/s]"}},"4e3f5ec6e2cd4105868bd34d729d0aa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbd712642f00447085d396fceaf1f3f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e10d3367bdd441a832509299e59438d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37f137e40f864e73a79a1851e758c8d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665ee1e2d66a47a4a9128484f11d4280":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d42dc9d2f994146a3b9cd0bc3182a72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"586f1df46c8a40069afaf3395c2688ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2211674c980f4c2e92005734f0d0c0bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95d798da33864a73a3579ddf766e629c","IPY_MODEL_330e947040b24cfb8cb2eaf5184aa2a5","IPY_MODEL_fb1f493a037844cca4b229737fd67ee2"],"layout":"IPY_MODEL_c77a7179793e445bbac5789deb144c45"}},"95d798da33864a73a3579ddf766e629c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0fc4ffb8b8c43359d8586db9293a40e","placeholder":"​","style":"IPY_MODEL_57dfdcef7e834259a3991094bb0c4610","value":"Downloading pytorch_model.bin: 100%"}},"330e947040b24cfb8cb2eaf5184aa2a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea1975e57c9f490fa7d971202781dc07","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_953800fbc63a4458aa67938b84b85099","value":513302779}},"fb1f493a037844cca4b229737fd67ee2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b2261c57afe4a738a644bb5767aab69","placeholder":"​","style":"IPY_MODEL_c36ad76a93f94a66b423765febdee0d4","value":" 513M/513M [00:01&lt;00:00, 298MB/s]"}},"c77a7179793e445bbac5789deb144c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0fc4ffb8b8c43359d8586db9293a40e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57dfdcef7e834259a3991094bb0c4610":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea1975e57c9f490fa7d971202781dc07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"953800fbc63a4458aa67938b84b85099":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b2261c57afe4a738a644bb5767aab69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c36ad76a93f94a66b423765febdee0d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z2LGcoawrlQ7"},"source":["# 0. Install Packages"]},{"cell_type":"code","metadata":{"id":"9xwaztFKrmDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687913717017,"user_tz":-540,"elapsed":9394,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"02a6922e-4e8f-4215-9fbe-9615d0bddce1"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"2TQx6kl-rkdM"},"source":["# 1. Import Packages"]},{"cell_type":"markdown","metadata":{"id":"YuKXZmdcrkdS"},"source":[" - 본 실습에 필요한 패키지들을 불러옵니다."]},{"cell_type":"code","metadata":{"id":"fR-OY7uhrkdS","executionInfo":{"status":"ok","timestamp":1687913721783,"user_tz":-540,"elapsed":4775,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["from transformers import GPT2Model\n","from transformers import GPT2LMHeadModel\n","from transformers import PreTrainedTokenizerFast\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import urllib\n","import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvObBbHnrkdT"},"source":["# 2. KoGPT2 Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"PEiF3thorkdU"},"source":[" - 사전 학습된 KoGPT2 Tokenizer를 불러옵니다."]},{"cell_type":"code","metadata":{"id":"mn3NQZyrrkdV","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["9dd77294a36147a0b1ab8962089de8d9","39daaa9b86b84679ba5241af58c842eb","2df640a766034476aab9dc1a42ae8c62","ac7b1cc8ac244f4faafb41044f35bb06","0933782cc86f471e8ef55984f1b919de","c3c62caa311c4cdfadb43af95fedc720","709527935c8c461fa3f3e1161c1a3241","0112c36948ad413bbcb9d57240db2b9f","97719df472964939b1944d5ee22d3817","c2d9c865b2854e32927bd5765eac9f82","28ca4995e46040aab3cb085f0472b5b5","674e2623d1564f09a92c65f38c9a2ed2","26b0e329808c482ba7008b847467f894","960093b1dbb644b89dc12c0648946a17","688d4209618347058df788b68227e114","4e3f5ec6e2cd4105868bd34d729d0aa4","dbd712642f00447085d396fceaf1f3f5","5e10d3367bdd441a832509299e59438d","37f137e40f864e73a79a1851e758c8d2","665ee1e2d66a47a4a9128484f11d4280","6d42dc9d2f994146a3b9cd0bc3182a72","586f1df46c8a40069afaf3395c2688ea"]},"executionInfo":{"status":"ok","timestamp":1687913727649,"user_tz":-540,"elapsed":2608,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"8a832d99-4cfd-45d5-a892-4c01db7e9ffc"},"source":["tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='right')\n","sample_text = \"근육이 커지기 위해서는\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","print(f' Sentence: {sample_text}')\n","print(f'   Tokens: {tokens}')\n","print(f'Token IDs: {token_ids}')"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd77294a36147a0b1ab8962089de8d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674e2623d1564f09a92c65f38c9a2ed2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":[" Sentence: 근육이 커지기 위해서는\n","   Tokens: ['▁근육이', '▁커', '지기', '▁위해서는']\n","Token IDs: [33245, 10114, 12748, 11357]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Iyd2Ya1RrkdW"},"source":["# 3. KoGPT2 Models"]},{"cell_type":"markdown","metadata":{"id":"q7O35G28rkdW"},"source":[" - GPT2Model과 GPT2LMHeadModel을 불러옵니다."]},{"cell_type":"markdown","metadata":{"id":"vqeeLgoQrkdW"},"source":["## 3-1. GPT2Model"]},{"cell_type":"markdown","metadata":{"id":"YvrfHXB6rkdX"},"source":[" - GPT2Model은 hidden state를 출력합니다.\n","\n"," - 본 예제에서는 네 개의 토큰에 대한 768차원의 벡터가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"NiWFyvYLrkdX","colab":{"base_uri":"https://localhost:8080/","height":144,"referenced_widgets":["2211674c980f4c2e92005734f0d0c0bf","95d798da33864a73a3579ddf766e629c","330e947040b24cfb8cb2eaf5184aa2a5","fb1f493a037844cca4b229737fd67ee2","c77a7179793e445bbac5789deb144c45","c0fc4ffb8b8c43359d8586db9293a40e","57dfdcef7e834259a3991094bb0c4610","ea1975e57c9f490fa7d971202781dc07","953800fbc63a4458aa67938b84b85099","3b2261c57afe4a738a644bb5767aab69","c36ad76a93f94a66b423765febdee0d4"]},"executionInfo":{"status":"ok","timestamp":1687913878930,"user_tz":-540,"elapsed":5150,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"7675a791-9d48-4f57-f380-08ec1ae1269c"},"source":["gpt2_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","hidden_states = gpt2_model(torch.tensor([token_ids]))\n","last_hidden_state = hidden_states[0]\n","print(last_hidden_state.shape)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2211674c980f4c2e92005734f0d0c0bf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"PUbrSbg9rkdX"},"source":["## 3-2. GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{"id":"KM2GGr6prkdX"},"source":[" - GPT2LMHead는 next word prediction을 출력합니다.\n","\n"," - 본 예제에서는 네 개의 토큰에 대한 51200 차원의 단어 확률 분포가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"cFyDm2aprkdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687914349531,"user_tz":-540,"elapsed":3228,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"d67e0ec5-2055-48a9-d2d7-95183c0dfcd2"},"source":["gpt2lm_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","outputs = gpt2lm_model(torch.tensor([token_ids]))\n","next_word_predictions = outputs[0]\n","print(next_word_predictions.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 51200])\n"]}]},{"cell_type":"code","source":["next_word_predictions[0][-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pT7Y4iEihNJ","executionInfo":{"status":"ok","timestamp":1687914438467,"user_tz":-540,"elapsed":626,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"d9580b23-1c25-4781-c3e1-741a16255fca"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-5.4558, -6.0249, -6.1399,  ..., -0.2245, -4.0262, -3.2469],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"L2hAXUowrkdY"},"source":[" - 단어 확률 분포에 대해 argmax를 취해 가장 높은 확률을 보이는 단어를 찾습니다.\n","\n"," - 본 예제에서는 \"무엇보다\" 라는 단어가 가장 높은 확률을 나타냅니다."]},{"cell_type":"code","metadata":{"id":"MXKCYYAIrkdY"},"source":["next_word_distribution = next_word_predictions[0, -1, :]\n","next_word_id = torch.argmax(next_word_distribution)\n","next_word = tokenizer.decode(next_word_id)\n","\n","print(f'Next word: {next_word}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dm63zr0yrkdZ"},"source":["# 4. Text Generation Examples (Pre-trained model)"]},{"cell_type":"markdown","metadata":{"id":"eC7r6SZorkdZ"},"source":[" - 두 가지 Text Generation 방법을 실험해봅니다."]},{"cell_type":"markdown","metadata":{"id":"NSAms2NyrkdZ"},"source":["## 4-1. Greedy Search"]},{"cell_type":"markdown","metadata":{"id":"Nzw0MSrhrkdZ"},"source":[" - Greedy Search는 가장 높은 확률의 단어를 Greedy하게 찾는 방식으로 텍스트를 생성합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"DQcKnXhUrkdZ"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           )\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hB3dzvRRrkda"},"source":["## 4-2. Beam Search"]},{"cell_type":"markdown","metadata":{"id":"VYeC4nggrkda"},"source":[" - Beam Search는 매 step마다 num_beams 개 만큼의 Top word selection path를 찾습니다.\n"]},{"cell_type":"code","metadata":{"id":"2NxnOmsg1viV"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           num_beams=5,\n","                           )\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EeNelLxerkda"},"source":["# 5. Fine tunning (Naver Movie review)"]},{"cell_type":"markdown","metadata":{"id":"t4MoKU6nrkda"},"source":[" - 네이버 영화 리뷰데이터를 활용하여 모델을 Fine Tuning 합니다."]},{"cell_type":"markdown","metadata":{"id":"mREdF7iarkda"},"source":["## 5-1. Get Datasets"]},{"cell_type":"markdown","metadata":{"id":"gJre_mPgrkda"},"source":[" - github으로부터 네이버 영화 리뷰데이터를 요청하여 내 pc에 저장합니다.\n","\n"," - 데이터의 크기가 너무 큰 관계로, 본 실험에서는 테스트 데이터 셋만을 활용하여 모델을 학습시킵니다."]},{"cell_type":"code","metadata":{"id":"xUdSUHaRrkdb"},"source":["def get_naver_review_examples():\n","    #urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n","\n","    #train_data = pd.read_table('ratings_train.txt')\n","    test_data = pd.read_table('ratings_test.txt')\n","\n","    return test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y-DCJAki786"},"source":["naver_data = get_naver_review_examples()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEw-UUV6i9Z-"},"source":["naver_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ljJUSVn3rkdb"},"source":[" - Dataset Loader를 정의합니다."]},{"cell_type":"code","metadata":{"id":"NmEdH-fdrkdb"},"source":["class NaverReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","          truncation=True,\n","        )\n","\n","        return {\n","          'text': text,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","    def __len__(self):\n","        return len(self.texts)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUcXquqfjZAn"},"source":["dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set[0]"],"metadata":{"id":"RrkzJ8KroI4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHXFcBEerkdb"},"source":["batch_size = 8\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_data = next(iter(test_dataloader))\n","sample_data"],"metadata":{"id":"KzN_EHt7op82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMj2_y_Zrkdc"},"source":["## 5-2. Model Settings"]},{"cell_type":"markdown","metadata":{"id":"MoEvXVKbrkdc"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"wn3YXDc5rkdc"},"source":["gpt2lm_model.train()\n","\n","learning_rate = 1e-5\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(gpt2lm_model.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 10\n","count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_inputs = sample_data['input_ids'].to(device)\n","sample_outputs = gpt2lm_model(sample_inputs, labels=sample_inputs)"],"metadata":{"id":"eZaZLaCqph3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_outputs['loss']"],"metadata":{"id":"dSAgM2yZp78r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m15km-r2rkdc"},"source":["## 5-3. Model Training"]},{"cell_type":"markdown","metadata":{"id":"58yCR-xarkdd"},"source":[" - Model의 학습을 시작합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iUoZ4cP8rkdd"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","        # train data를 모델에 입력하여 출력 값을 얻습니다.\n","        gpt2lm_model.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","        train_outputs = gpt2lm_model(train_inputs, labels=train_inputs) # train_outputs = (train_loss, train_logits, train_past_hidden_states)\n","\n","        train_loss, _ = train_outputs[:2]\n","\n","        # valid data를 모델에 입력하여 출력 값을 얻습니다.\n","\n","        valid_data = next(iter(valid_dataloader))\n","\n","        gpt2lm_model.to(device)\n","        valid_inputs = valid_data['input_ids'].to(device)\n","        valid_outputs = gpt2lm_model(valid_inputs, labels=valid_inputs)\n","\n","        valid_loss, _ = valid_outputs[:2]\n","\n","        gpt2lm_model.to(device)\n","\n","        # train loss를 토대로 모델을 학습합니다.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","\n","        # 200 batch 마다 학습 상황을 화면에 출력합니다.\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","\n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss))\n","\n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","\n","            count = 0\n","\n","            # 이전 valid_loss 보다 현재의 valid_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt2lm_model.state_dict(), f'./KoGPT-model.pth')\n","\n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpX6-BeE9NcR"},"source":["train_set[5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKp_ygcR8VhK"},"source":["kogpt_load_path = f\"./KoGPT-model.pth\"\n","\n","gpt2lm_model.load_state_dict(torch.load(kogpt_load_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVNd9zhPrkde"},"source":["gpt2lm_model.to(device)\n","\n","sample_text = \"정말 재미\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]).to(device),\n","                           max_length=127,\n","                           repetition_penalty=1.0,\n","                           num_beams=5)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Fs-HK23-BH4"},"source":["import re\n","\n","p = re.compile('<pad>')\n","re.sub(p, '', generated)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiDq9EVFrkde"},"source":["# 6. Fine Tuning 2 (Classification Task)"]},{"cell_type":"markdown","metadata":{"id":"j4VEfnherkde"},"source":[" - Dateset을 가져옵니다."]},{"cell_type":"code","metadata":{"id":"9Xcj-OEirkde"},"source":["tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzabGZvIrkde"},"source":["batch_size = 16\n","\n","naver_data = get_naver_review_examples()\n","\n","dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJPwD9Xerkde"},"source":[" - GPT Classifier를 정의합니다."]},{"cell_type":"code","metadata":{"id":"Wod_VKl7rkdf"},"source":["class GPT2SentimentClassifier(torch.nn.Module):\n","\n","    def __init__(self, n_classes):\n","        super(GPT2SentimentClassifier, self).__init__()\n","\n","        self.gpt_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","        self.drop = torch.nn.Dropout(p=0.1)\n","        self.out = torch.nn.Linear(self.gpt_model.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        hidden_states = self.gpt_model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","        last_hidden_state = hidden_states[0]\n","\n","        output = self.drop(last_hidden_state[:, -1, :])\n","\n","        return self.out(output)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBBgK4LDrkdf"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"KS8te_26rkdf"},"source":["gpt_clf = GPT2SentimentClassifier(n_classes=1)\n","gpt_clf.train()\n","\n","learning_rate = 5e-5\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(gpt_clf.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 1\n","count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zu6wBkkgrkdf"},"source":[" - 정확도 계산 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"bLNnDiNgrkdf"},"source":["def cal_correct_num(predicts, labels):\n","    predicts_ = predicts >= 0.5\n","    correct_num = torch.sum(predicts_ == labels)\n","\n","    return correct_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ad3KaQHCrkdg"},"source":[" - Model의 학습을 시작합니다"]},{"cell_type":"code","metadata":{"id":"38LkHRZ1rkdg"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","\n","train_correct_num = 0\n","valid_correct_num = 0\n","\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","        # train data를 모델에 입력하여 출력 값을 얻습니다.\n","        gpt_clf.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","        train_masks = train_data['attention_mask'].to(device)\n","        train_labels = train_data['labels'].to(device)\n","\n","        train_outputs = gpt_clf(train_inputs, train_masks)\n","\n","        train_loss = criterion(train_outputs.view(-1), train_labels.float())\n","\n","        # valid data를 모델에 입력하여 출력 값을 얻습니다.\n","\n","        valid_data = next(iter(valid_dataloader))\n","\n","        gpt_clf.to(device)\n","        valid_inputs = valid_data['input_ids'].to(device)\n","        valid_masks = valid_data['attention_mask'].to(device)\n","        valid_labels = valid_data['labels'].to(device)\n","\n","        valid_outputs = gpt_clf(valid_inputs, valid_masks)\n","\n","        valid_loss = criterion(valid_outputs.view(-1), valid_labels.float())\n","\n","        gpt_clf.to(device)\n","\n","        # train loss를 토대로 모델을 학습합니다.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","\n","        train_correct_num += cal_correct_num(torch.sigmoid(train_outputs.view(-1)), train_labels.float())\n","        valid_correct_num += cal_correct_num(torch.sigmoid(valid_outputs.view(-1)), valid_labels.float())\n","\n","        # 200 batch 마다 학습 상황을 화면에 출력합니다.\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","\n","            train_acc = train_correct_num / cnt\n","            valid_acc = valid_correct_num / cnt\n","\n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f | train_acc : %.5f | valid_acc : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss, train_acc, valid_acc))\n","\n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","\n","            train_correct_num = 0\n","            valid_correct_num = 0\n","\n","            count = 0\n","\n","            # 이전 test_loss 보다 현재의 test_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt_clf.state_dict(), f'./KoGPT-Classifier-model.pth')\n","\n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWY67d9Xrkdh"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aqpcaPSrkdh"},"source":[],"execution_count":null,"outputs":[]}]}
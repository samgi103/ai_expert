{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ed3754206f2349498567ce5a5c3605d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c04c34c01b984cbea6e114750274da05","IPY_MODEL_b2c9cd46c11545f7b2d0426d40502814","IPY_MODEL_5faaae1ade274cd0b4ee2c069e9f0eda"],"layout":"IPY_MODEL_7257b7feae484b0fb59192668b33ccf2"}},"c04c34c01b984cbea6e114750274da05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_045ca563c6114a64ba6b9a38eb245b70","placeholder":"​","style":"IPY_MODEL_f7997983c334419baf5e780d33840a90","value":"Downloading (…)/main/tokenizer.json: 100%"}},"b2c9cd46c11545f7b2d0426d40502814":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b3c56570f94d069a125bdfe83698bd","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f481eb68ff6646129a3a1c1e7de6f74e","value":2825034}},"5faaae1ade274cd0b4ee2c069e9f0eda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ebb62c87c294a8e9ff502a63ae78298","placeholder":"​","style":"IPY_MODEL_e46ba15f09b845de9a2befe558d64c24","value":" 2.83M/2.83M [00:00&lt;00:00, 6.08MB/s]"}},"7257b7feae484b0fb59192668b33ccf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"045ca563c6114a64ba6b9a38eb245b70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7997983c334419baf5e780d33840a90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40b3c56570f94d069a125bdfe83698bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f481eb68ff6646129a3a1c1e7de6f74e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ebb62c87c294a8e9ff502a63ae78298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e46ba15f09b845de9a2befe558d64c24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f4a855c92b942cea8052816421a1ece":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a566b685d0204ebf8282d96838388f60","IPY_MODEL_a5af5f5d51cf46ecb74d2b69336a5d96","IPY_MODEL_7638ee6b501e4257b8e6f28afe76cfa0"],"layout":"IPY_MODEL_a675836c8c724ca1b503d940810986cf"}},"a566b685d0204ebf8282d96838388f60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1f180f544c14557bbdd191f72cdca65","placeholder":"​","style":"IPY_MODEL_d30b79b8c1154b3bbb815cc15476aed1","value":"Downloading (…)lve/main/config.json: 100%"}},"a5af5f5d51cf46ecb74d2b69336a5d96":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aeb054c2cc14cb0988464466a483007","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8fc23ebe333d46ba9bfa3a807cdb8d36","value":1000}},"7638ee6b501e4257b8e6f28afe76cfa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e8080c4e3b14fd3bef90867335ff323","placeholder":"​","style":"IPY_MODEL_292c677f125548d3a8452a6086061c9a","value":" 1.00k/1.00k [00:00&lt;00:00, 25.0kB/s]"}},"a675836c8c724ca1b503d940810986cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1f180f544c14557bbdd191f72cdca65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d30b79b8c1154b3bbb815cc15476aed1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2aeb054c2cc14cb0988464466a483007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc23ebe333d46ba9bfa3a807cdb8d36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e8080c4e3b14fd3bef90867335ff323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"292c677f125548d3a8452a6086061c9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ecbed42cc354c2e82ac9934e08a8f6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_223c2f84ae104d98aee60b01c1a9a83a","IPY_MODEL_4f486084fcfb4409b9dea964fa959b21","IPY_MODEL_6db6e4bc819240a594d77e6f042044cc"],"layout":"IPY_MODEL_8642f37874784ce687932b99aca70e54"}},"223c2f84ae104d98aee60b01c1a9a83a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd35b074e0f14e54b371bfee1c9c25ba","placeholder":"​","style":"IPY_MODEL_39c5f4ea2f724f20b68af3a886ffc82b","value":"Downloading pytorch_model.bin: 100%"}},"4f486084fcfb4409b9dea964fa959b21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc3f2c119e5040079b7e066c11121d6c","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c231623b52b840908cad9dd95806e62f","value":513302779}},"6db6e4bc819240a594d77e6f042044cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ca641622ad342178203e16a8953ea8e","placeholder":"​","style":"IPY_MODEL_22bb6eca850945469e61f200c29152d1","value":" 513M/513M [00:03&lt;00:00, 163MB/s]"}},"8642f37874784ce687932b99aca70e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd35b074e0f14e54b371bfee1c9c25ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39c5f4ea2f724f20b68af3a886ffc82b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc3f2c119e5040079b7e066c11121d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c231623b52b840908cad9dd95806e62f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ca641622ad342178203e16a8953ea8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22bb6eca850945469e61f200c29152d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Kd66-f6FqYS-"},"source":["# 0. Install Packages"]},{"cell_type":"code","metadata":{"id":"Beu0JUFfpNDF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687914770460,"user_tz":-540,"elapsed":13921,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"8429f41f-27f7-4b19-fe8b-bb7cd4c0f393"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"gTRk8xWlpAz-"},"source":["# 1. Import Packages"]},{"cell_type":"markdown","metadata":{"id":"JWnd0ohwpA0E"},"source":[" - 본 실습에 필요한 패키지들을 불러옵니다."]},{"cell_type":"code","metadata":{"id":"ENsigHJQpA0F","executionInfo":{"status":"ok","timestamp":1687914791105,"user_tz":-540,"elapsed":10021,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["from transformers import GPT2Model\n","from transformers import GPT2LMHeadModel\n","from transformers import PreTrainedTokenizerFast\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import urllib\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKp5HlAypA0G"},"source":["# 2. KoGPT2 Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"gz-lSpbPpA0G"},"source":[" - 사전 학습된 KoGPT2 Tokenizer를 불러옵니다."]},{"cell_type":"code","metadata":{"id":"GNcRV1kVpA0H","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["ed3754206f2349498567ce5a5c3605d1","c04c34c01b984cbea6e114750274da05","b2c9cd46c11545f7b2d0426d40502814","5faaae1ade274cd0b4ee2c069e9f0eda","7257b7feae484b0fb59192668b33ccf2","045ca563c6114a64ba6b9a38eb245b70","f7997983c334419baf5e780d33840a90","40b3c56570f94d069a125bdfe83698bd","f481eb68ff6646129a3a1c1e7de6f74e","0ebb62c87c294a8e9ff502a63ae78298","e46ba15f09b845de9a2befe558d64c24","9f4a855c92b942cea8052816421a1ece","a566b685d0204ebf8282d96838388f60","a5af5f5d51cf46ecb74d2b69336a5d96","7638ee6b501e4257b8e6f28afe76cfa0","a675836c8c724ca1b503d940810986cf","c1f180f544c14557bbdd191f72cdca65","d30b79b8c1154b3bbb815cc15476aed1","2aeb054c2cc14cb0988464466a483007","8fc23ebe333d46ba9bfa3a807cdb8d36","1e8080c4e3b14fd3bef90867335ff323","292c677f125548d3a8452a6086061c9a"]},"executionInfo":{"status":"ok","timestamp":1687914796590,"user_tz":-540,"elapsed":1710,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"7f7a9eea-44a7-4536-aa87-224fc8009310"},"source":["tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='right')\n","sample_text = \"근육이 커지기 위해서는\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","print(f' Sentence: {sample_text}')\n","print(f'   Tokens: {tokens}')\n","print(f'Token IDs: {token_ids}')"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3754206f2349498567ce5a5c3605d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4a855c92b942cea8052816421a1ece"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":[" Sentence: 근육이 커지기 위해서는\n","   Tokens: ['▁근육이', '▁커', '지기', '▁위해서는']\n","Token IDs: [33245, 10114, 12748, 11357]\n"]}]},{"cell_type":"markdown","metadata":{"id":"pAu2WxaRpA0H"},"source":["# 3. KoGPT2 Models"]},{"cell_type":"markdown","metadata":{"id":"YTaFqhgvpA0H"},"source":[" - GPT2Model과 GPT2LMHeadModel을 불러옵니다."]},{"cell_type":"markdown","metadata":{"id":"QsI1nriYpA0I"},"source":["## 3-1. GPT2Model"]},{"cell_type":"markdown","metadata":{"id":"lpIW6ZKEpA0I"},"source":[" - GPT2Model은 hidden state를 출력합니다.\n","\n"," - 본 예제에서는 네 개의 토큰에 대한 768차원의 벡터가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"-WAcIkMVpA0I","colab":{"base_uri":"https://localhost:8080/","height":144,"referenced_widgets":["2ecbed42cc354c2e82ac9934e08a8f6a","223c2f84ae104d98aee60b01c1a9a83a","4f486084fcfb4409b9dea964fa959b21","6db6e4bc819240a594d77e6f042044cc","8642f37874784ce687932b99aca70e54","bd35b074e0f14e54b371bfee1c9c25ba","39c5f4ea2f724f20b68af3a886ffc82b","dc3f2c119e5040079b7e066c11121d6c","c231623b52b840908cad9dd95806e62f","0ca641622ad342178203e16a8953ea8e","22bb6eca850945469e61f200c29152d1"]},"executionInfo":{"status":"ok","timestamp":1687914811154,"user_tz":-540,"elapsed":6587,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"8937b695-7b0b-4ae7-e388-17ddf0d26604"},"source":["gpt2_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","hidden_states = gpt2_model(torch.tensor([token_ids]))\n","last_hidden_state = hidden_states[0]\n","print(last_hidden_state.shape)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ecbed42cc354c2e82ac9934e08a8f6a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"iw40kYSSpA0I"},"source":["## 3-2. GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{"id":"3gSt2CFrpA0J"},"source":[" - GPT2LMHead는 next word prediction을 출력합니다.\n","\n"," - 본 예제에서는 네 개의 토큰에 대한 51200 차원의 단어 확률 분포가 도출됩니다."]},{"cell_type":"code","metadata":{"id":"UN-H6_b3pA0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687914817727,"user_tz":-540,"elapsed":3372,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"33098b9d-cf86-42a2-8955-09ff6c35022f"},"source":["gpt2lm_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","outputs = gpt2lm_model(torch.tensor([token_ids]))\n","next_word_predictions = outputs[0]\n","print(next_word_predictions.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 51200])\n"]}]},{"cell_type":"markdown","metadata":{"id":"cG3ZRXl6pA0J"},"source":[" - 단어 확률 분포에 대해 가장 높은 확률을 보이는 단어를 찾습니다.\n","\n"," - 본 예제에서는 \"무엇보다\" 라는 단어가 가장 높은 확률을 나타냅니다."]},{"cell_type":"code","metadata":{"id":"h7r4cIPmpA0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687915103317,"user_tz":-540,"elapsed":365,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"10c40b36-b5c7-4e35-f15f-05468212ae38"},"source":["next_word_distribution = next_word_predictions[0, -1, :]\n","\n","# To-do\n","# next_word_id = ??? # 가장 높은 확률을 보이는 단어 id를 찾으세요.\n","# next_word = ??? # 해당 id를 토대로 실제 단어를 도출하세요. \"무엇보다\"\n","next_word_id = torch.argmax(next_word_distribution)\n","next_word = tokenizer.decode(next_word_id)\n","\n","print(f'Next word: {next_word}')"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Next word: 무엇보다\n"]}]},{"cell_type":"code","source":["next_word_predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjQ2xVD3lOmw","executionInfo":{"status":"ok","timestamp":1687915131872,"user_tz":-540,"elapsed":368,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"e83b61a8-91a4-43eb-f621-5f0b7342e918"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4, 51200])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["next_word_distribution"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL3_wyAuk5ER","executionInfo":{"status":"ok","timestamp":1687915133428,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"d7be246d-7154-4a0f-94c1-a2bf5e580517"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-5.4558, -6.0249, -6.1399,  ..., -0.2245, -4.0262, -3.2469],\n","       grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"TcxFemozpA0K"},"source":["# 4. Text Generation Examples (Pre-trained model)"]},{"cell_type":"markdown","metadata":{"id":"i74IHrg6pA0K"},"source":[" - 두 가지 Text Generation 방법을 실험해봅니다."]},{"cell_type":"markdown","metadata":{"id":"LZhwL0oBpA0K"},"source":["## 4-1. Greedy Search"]},{"cell_type":"markdown","metadata":{"id":"6O_B5NHapA0K"},"source":[" - Greedy Search는 가장 높은 확률의 단어를 Greedy하게 찾는 방식으로 텍스트를 생성합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Dc2b3SaEpA0K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687917111879,"user_tz":-540,"elapsed":7192,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"3829e682-a26a-4442-c69f-76f28318d351"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n","특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n","또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n","아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n","운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n","운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n","\n"]}]},{"cell_type":"code","source":["gen_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6LxDolRtbl2","executionInfo":{"status":"ok","timestamp":1687917285094,"user_tz":-540,"elapsed":8,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"ff4f429a-4688-431c-aec2-50aac09fbed2"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[33245, 10114, 12748, 11357, 14564,  9238, 33215,  6867,  6903, 11405,\n","         12155,  8610,  8143,  9091,  9377, 12486, 10320,  9199, 15177, 27459,\n","          6867,  8135, 48240,  9731,  9492, 19870,  9280,  9650, 10254,  9178,\n","          8146,  7397,  8704, 33215, 42250,  9091,  9377, 12486, 12826, 11357,\n","         39713, 18595,  9025, 11189, 10474,  8711,  8244,  7991,  9178,  7335,\n","          8704, 41046, 10558,  8382,  9124,  9922, 10108, 10090,  9049,  7884,\n","          9743, 16364, 17764,  9033, 10599, 13229,  9508,  8694, 10002, 12443,\n","          8137, 10474, 18566,  9049,  7884,  8196,  9460,  9479,  7803,  7813,\n","          9114, 13626,  9029,  8022,  7803,  7813,  9114, 16373, 15386, 38625,\n","          9752,   387, 10403,  6824, 26523, 16255,  7249,  9341, 47036,  7607,\n","          9844, 12972, 10764,  9243, 10552,  9029, 26421,  9479,  7803,  7813,\n","          9114, 16373, 41845, 18418,  9277, 13576,  7759, 10115, 15231,  8711,\n","         14564,  9153,  9823, 13508, 11469, 12972, 12364]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"LCIkzLOJpA0L"},"source":["## 4-2. Beam Search"]},{"cell_type":"markdown","metadata":{"id":"Rf3bqIMwpA0L"},"source":[" - Beam Search는 매 step마다 num_beams 개 만큼의 Top word selection path를 찾습니다."]},{"cell_type":"code","metadata":{"id":"gPStdgAspA0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687917352348,"user_tz":-540,"elapsed":15613,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"35128510-4ec6-41e0-9961-eeba295ebc8d"},"source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           num_beams=5)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["근육이 커지기 위해서는 피부 속 콜라겐과 엘라스틴의 생성을 촉진시키는 것이 중요하다.\n","콜라겐은 피부의 탄력을 유지하는 데 중요한 역할을 한다.\n","이러한 콜라겐의 생성을 촉진시키기 위해서는 피부에 충분한 수분을 공급해줘야 한다.\n","또한 피부를 촉촉하게 유지시켜주는 보습제를 꾸준히 섭취하는 것도 도움이 된다.\n","피부에 영양을 공급해주는 보습제로는 에센셜 오일이 있다.\n","에센셜 오일은 비타민 A, C, E가 풍부하게 함유돼 있어 노화방지에 도움을 주는 것으로 알려져 있다.\n","특히 에센셜 오일은 항산화 작용을 하는 활성산소를 억제해 피부 노화를 방지하는데 도움을 준다.\n","\n"]}]},{"cell_type":"code","source":["gen_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N85aOoX8txFN","executionInfo":{"status":"ok","timestamp":1687917372333,"user_tz":-540,"elapsed":2,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"f51325dd-75f4-45a9-e65e-4061bfabf70d"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[33245, 10114, 12748, 11357, 14564,  9238, 33215,  6867,  6903, 11405,\n","         12155,  8610,  8143,  9091,  9377, 12486, 10320,  9199, 15177, 27459,\n","          6867,  8135, 48240,  9731,  9492, 19870,  9280,  9650, 10254,  9178,\n","          8146,  7397,  8704, 33215, 42250,  9091,  9377, 12486, 12826, 11357,\n","         39713, 18595,  9025, 11189, 10474,  8711,  8244,  7991,  9178,  7335,\n","          8704, 41046, 10558,  8382,  9124,  9922, 10108, 10090,  9049,  7884,\n","          9743, 16364, 17764,  9033, 10599, 13229,  9508,  8694, 10002, 12443,\n","          8137, 10474, 18566,  9049,  7884,  8196,  9460,  9479,  7803,  7813,\n","          9114, 13626,  9029,  8022,  7803,  7813,  9114, 16373, 15386, 38625,\n","          9752,   387, 10403,  6824, 26523, 16255,  7249,  9341, 47036,  7607,\n","          9844, 12972, 10764,  9243, 10552,  9029, 26421,  9479,  7803,  7813,\n","          9114, 16373, 41845, 18418,  9277, 13576,  7759, 10115, 15231,  8711,\n","         14564,  9153,  9823, 13508, 11469, 12972, 12364]])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]),\n","                           max_length=127,\n","                           repetition_penalty=2.0,\n","                           num_beams=1)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVers2J5uIes","executionInfo":{"status":"ok","timestamp":1687917598670,"user_tz":-540,"elapsed":9192,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"1a9cbb41-5035-4e28-84da-87476ba488d6"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n","특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n","또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n","아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n","운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n","운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"e-KW0fyKpA0L"},"source":["# 5. Fine tunning 1 (Naver Movie review)"]},{"cell_type":"markdown","metadata":{"id":"SPXmH1pypA0M"},"source":[" - 네이버 영화 리뷰데이터를 활용하여 모델을 Fine Tuning 합니다."]},{"cell_type":"markdown","metadata":{"id":"wS7BvE1zpA0M"},"source":["## 5-1. Get Datasets"]},{"cell_type":"markdown","metadata":{"id":"feYvCV1UpA0M"},"source":[" - github으로부터 네이버 영화 리뷰데이터를 요청하여 내 pc에 저장합니다.\n","\n"," - 데이터의 크기가 너무 큰 관계로, 본 실험에서는 테스트 데이터 셋만을 활용하여 모델을 학습시킵니다."]},{"cell_type":"code","metadata":{"id":"2HoQTkWppA0M","executionInfo":{"status":"ok","timestamp":1687917775112,"user_tz":-540,"elapsed":433,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["def get_naver_review_examples():\n","    #urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n","\n","    #train_data = pd.read_table('ratings_train.txt')\n","    test_data = pd.read_table('ratings_test.txt')\n","\n","    return test_data"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6HpJ_3KqIY2","executionInfo":{"status":"ok","timestamp":1687917779112,"user_tz":-540,"elapsed":431,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["naver_data = get_naver_review_examples()"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tRFniGZqIil","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1687917780786,"user_tz":-540,"elapsed":4,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"27553e06-227e-45e2-9f18-9a1967bb4d48"},"source":["naver_data"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            id                                           document  label\n","0      6270596                                                굳 ㅋ      1\n","1      9274899                               GDNTOPCLASSINTHECLUB      0\n","2      8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n","3      6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n","4      6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n","...        ...                                                ...    ...\n","49995  4608761          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n","49996  5308387       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n","49997  9072549                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n","49998  5802125     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n","49999  6070594                                         마무리는 또 왜이래      0\n","\n","[50000 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-8331029d-4032-42da-8663-5b5a403bbde8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6270596</td>\n","      <td>굳 ㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9274899</td>\n","      <td>GDNTOPCLASSINTHECLUB</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8544678</td>\n","      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6825595</td>\n","      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6723715</td>\n","      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>4608761</td>\n","      <td>오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>5308387</td>\n","      <td>의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>9072549</td>\n","      <td>그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>5802125</td>\n","      <td>절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>6070594</td>\n","      <td>마무리는 또 왜이래</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8331029d-4032-42da-8663-5b5a403bbde8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8331029d-4032-42da-8663-5b5a403bbde8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8331029d-4032-42da-8663-5b5a403bbde8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"2XwQTo_qpA0M"},"source":[" - Dataset Loader를 정의합니다."]},{"cell_type":"code","metadata":{"id":"2fp3f5RPpA0N","executionInfo":{"status":"ok","timestamp":1687918122554,"user_tz":-540,"elapsed":433,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["class NaverReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","          truncation=True,\n","        )\n","\n","        return {\n","          'text': text,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","    def __len__(self):\n","        return len(self.texts)\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"8caoyKAQqMVy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687918131667,"user_tz":-540,"elapsed":450,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"b60901eb-bfa2-43cc-d778-5ed39664c3cf"},"source":["dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])\n","train_set[0]"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': '이럴줄 알았다. 진짜 유치해 죽을뻔.이게 명작임? 아니 진짜 유치해 토막될뻔',\n"," 'input_ids': tensor([ 9018,  7400,  8239,  9181, 32574, 23971, 13215,  8711, 14909,  7724,\n","           389, 13421,  9170,  8160,  8152,   406,  9320, 23971, 13215,  8711,\n","         36788,  7253,  7724,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]),\n"," 'labels': tensor(0)}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"cZMuocOQpA0N","executionInfo":{"status":"ok","timestamp":1687918462814,"user_tz":-540,"elapsed":409,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["batch_size = 4\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","# 일반적으로 valid와 test는 shuffle을 안하는게 좋음\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=False)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=False)"],"execution_count":29,"outputs":[]},{"cell_type":"code","source":["sample_data = next(iter(test_dataloader))\n","sample_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fBOFjGRyEZj","executionInfo":{"status":"ok","timestamp":1687918600152,"user_tz":-540,"elapsed":425,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"37f95d4c-f46d-46fd-9a64-6301a83678dc"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ['이 따위도 영화야?...',\n","  '전반적으로 초반부터 흥미진진해서 재밋었던영화 ,ㅎㅎ 추천~',\n","  '시즌3의 실패는 카리스마 음악의 진실의 깊이를 모두 갖은 김기련',\n","  '다시는 못느낄 어릴 적 그때의 신비로움.판타지...'],\n"," 'input_ids': tensor([[ 9018,  9106, 22881, 10584,  7991,   406, 29045,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n","         [20090, 18622,  9148, 15112,  8265,  8265,  9867,  9150,  7591, 12016,\n","          14558, 11481,   608,   608, 13815,   468,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n","         [30935,   394,  8143, 11478,  7162,  9488, 10191,  7487, 21376,  9166,\n","          15555, 39813,  9432, 45183, 26475,  7417,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n","         [29810,  9350,  7160,  7051, 19270,  9211, 49813,  9084, 14229,  8098,\n","            389,  8618,  8519,  8263, 29045,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n","              3,     3,     3,     3,     3,     3,     3,     3,     3,     3]]),\n"," 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0]]),\n"," 'labels': tensor([0, 1, 0, 1])}"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"Gpy5IdgXpA0O"},"source":["## 5-2. Model Settings"]},{"cell_type":"markdown","metadata":{"id":"LVYfIrqYpA0O"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"62Lp9LzmpA0O","executionInfo":{"status":"ok","timestamp":1687918533226,"user_tz":-540,"elapsed":430,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"source":["gpt2lm_model.train()\n","\n","learning_rate = 3e-5\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(gpt2lm_model.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 10\n","count = 0"],"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# list(gpt2lm_model.parameters())[0][1].shape\n","list(gpt2lm_model.named_parameters())[0][1].shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZjYL2JRyN9M","executionInfo":{"status":"ok","timestamp":1687918643573,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"e429ba17-8ed4-466a-cd64-dd5f4fedf598"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([51200, 768])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["total_params = list(gpt2lm_model.parameters())\n","user_defined_parameters = total_params[1:]\n","torch.optim.Adam(user_defined_parameters, lr=learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJFVG-uPyqNE","executionInfo":{"status":"ok","timestamp":1687918727141,"user_tz":-540,"elapsed":3,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"ae798195-45cb-4536-9753-80d69053e97a"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    lr: 3e-05\n","    maximize: False\n","    weight_decay: 0\n",")"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["device = 'cuda'\n","gpt2lm_model.to(device)\n","sample_inputs = sample_data['input_ids'].to(device)\n","sample_outputs = gpt2lm_model(sample_inputs, labels = sample_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"NiMYeVYqy9vs","executionInfo":{"status":"error","timestamp":1687919616556,"user_tz":-540,"elapsed":511,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"fccb0fad-2612-49bb-ceb7-d7183eb63f6a"},"execution_count":40,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-dc61edb034e0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# device = 'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# gpt2lm_model.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2lm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":["sample_outputs.keys()"],"metadata":{"id":"GlHdKEjk2hgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_outputs['loss']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"rba2z1lJzMHj","executionInfo":{"status":"error","timestamp":1687918797753,"user_tz":-540,"elapsed":579,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"55ca0994-e17f-47f8-a923-be9c881aa8cb"},"execution_count":39,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-98300d592b86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'sample_outputs' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"r5JGnchqpA0O"},"source":["## 5-3. Model Training"]},{"cell_type":"markdown","metadata":{"id":"BHDibxzspA0P"},"source":[" - Model의 학습을 시작합니다."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"nxbpMqQkpA0P"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","\n","        gpt2lm_model.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","\n","        # To-do\n","        # train_outputs = ??? # gpt2lm_model을 통해 다음에 나올 단어를 예측해보세요.\n","        # train_loss = ??? # train_outputs을 활용하여 train_loss를 계산하세요.\n","        train_outputs = gpt2lm_model(train_inputs, labels = train_inputs)\n","        train_loss, _ = train_outputs[:2]\n","\n","\n","        valid_data = next(iter(valid_dataloader))\n","\n","        valid_inputs = valid_data['input_ids'].to(device)\n","\n","        # To-do\n","        # valid_outputs = ??? # gpt2lm_model을 통해 다음에 나올 단어를 예측해보세요.\n","        # valid_loss = ??? # valid_outputs을 활용하여 train_loss를 계산하세요.\n","        valid_outputs = gpt2lm_model(valid_inputs, labels = valid_inputs)\n","        valid_loss, _ = valid_outputs[:2]\n","\n","\n","        # To-do\n","        # Gradients를 0으로 초기화하세요.\n","        # Back-propagation을 통해 Gradients를 계산하세요.\n","        # 계산된 Gradients를 통해 Parameter를 업데이트하세요.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","\n","\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","\n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss))\n","\n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","\n","            count = 0\n","\n","            # 이전 test_loss 보다 현재의 test_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt2lm_model.state_dict(), f'./KoGPT-model.pth')\n","\n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set[5]"],"metadata":{"id":"hitpN7CE8MQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kogpt_load_path = f\"./KoGPT-model.pth\"\n","\n","gpt2lm_model.load_state_dict(torch.load(kogpt_load_path))"],"metadata":{"id":"n05UNWy68Nrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2lm_model.to(device)\n","\n","sample_text = \"정말 재미\"\n","\n","tokens = tokenizer.tokenize(sample_text)\n","token_ids = tokenizer.encode(sample_text)\n","\n","gen_ids = gpt2lm_model.generate(torch.tensor([token_ids]).to(device),\n","                           max_length=127,\n","                           repetition_penalty=1.0,\n","                           num_beams=5)\n","\n","generated = tokenizer.decode(gen_ids[0,:].tolist())\n","print(generated)"],"metadata":{"id":"5MhEt-XE8O_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","p = re.compile('<pad>')\n","re.sub(p, '', generated)"],"metadata":{"id":"GQTpxqkw8RFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFvTdYVipA0P"},"source":["# 6. Fine Tuning 2 (Classification Task)"]},{"cell_type":"markdown","metadata":{"id":"UPxYbuospA0P"},"source":[" - Dateset을 가져옵니다."]},{"cell_type":"code","metadata":{"id":"5EIX7qutpA0P"},"source":["# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='right')\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', padding_side='left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0Vp4mTEpA0Q"},"source":["batch_size = 16\n","\n","naver_data = get_naver_review_examples()\n","\n","dataset = NaverReviewDataset(naver_data['document'], naver_data['label'], tokenizer, 100)\n","train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [40000, 5000, 5000])\n","\n","train_dataloader = DataLoader(train_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","valid_dataloader = DataLoader(valid_set, batch_size=batch_size,\n","                        shuffle=True)\n","\n","test_dataloader = DataLoader(test_set, batch_size=batch_size,\n","                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UiY8bTqQpA0Q"},"source":[" - GPT Classifier를 정의합니다."]},{"cell_type":"code","metadata":{"id":"N-n-WPmTpA0Q"},"source":["class GPT2SentimentClassifier(torch.nn.Module):\n","\n","    def __init__(self, n_classes):\n","        super(GPT2SentimentClassifier, self).__init__()\n","\n","        self.gpt_model = GPT2Model.from_pretrained('skt/kogpt2-base-v2')\n","\n","        # To-do\n","        # 감정 분류를 위한 Layer들을 정의하세요.\n","        self.drop = torch.nn.Dropout(p=0.1)\n","        self.out = torch.nn.Linear(self.gpt_model.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","\n","        # To-do\n","        # 감정 분류 테스크를 위한 코드를 작성하세요.\n","        hidden_states = self.gpt_model(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask\n","        )\n","        last_hidden_state = hidden_states[0]\n","\n","        output = self.drop(last_hidden_state[:, -1, :])\n","\n","        return self.out(output)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q6g5PFeCpA0Q"},"source":[" - Model의 환경을 설정합니다."]},{"cell_type":"code","metadata":{"id":"8i180P1hpA0Q"},"source":["gpt_clf = GPT2SentimentClassifier(n_classes=1)\n","gpt_clf.train()\n","\n","pre_trained_lr = 1e-5\n","lr = 3e-5\n","\n","# To-do\n","# criterion = ??? # 이진 분류를 위한 손실 함수를 정의하세요.\n","# optimizer = ??? # Adam Optimizer를 활용하여 Pre-trained layer는 1e-5, 새로 추가한 layer는 3e-5의 learning rate를 부여하세요.\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(gpt_clf.parameters(), lr=learning_rate)\n","\n","device = 'cuda'\n","\n","epochs = 1\n","count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZqEcdR6pA0R"},"source":["- 정확도 계산 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"9BovX9mrpA0R"},"source":["def cal_correct_num(predicts, labels):\n","    predicts_ = predicts >= 0.5\n","    correct_num = torch.sum(predicts_ == labels)\n","\n","    return correct_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSOg9S_9pA0R"},"source":[" - Model의 학습을 시작합니다"]},{"cell_type":"code","metadata":{"id":"cOy4_q5NpA0R"},"source":["tot_train_loss = 0.0\n","tot_valid_loss = 0.0\n","\n","train_correct_num = 0\n","valid_correct_num = 0\n","\n","prev_valid_loss = 10000\n","\n","print('KoGPT-2 Training Start!')\n","\n","for epoch in range(epochs):\n","    for batch, train_data in enumerate(train_dataloader):\n","        gpt_clf.to(device)\n","        train_inputs = train_data['input_ids'].to(device)\n","        train_masks = train_data['attention_mask'].to(device)\n","        train_labels = train_data['labels'].to(device)\n","\n","        # train_outputs = ??? # 입력 값들을 할당하여 감정을 예측해보세요.\n","        # train_loss = ??? # train_outputs를 활용하여 train_loss를 계산하세요.\n","        train_outputs = gpt_clf(train_inputs, train_masks)\n","        train_loss = criterion(train_outputs.view(-1), train_labels.float())\n","\n","\n","\n","        valid_data = next(iter(valid_dataloader))\n","\n","        valid_inputs = valid_data['input_ids'].to(device)\n","        valid_masks = valid_data['attention_mask'].to(device)\n","        valid_labels = valid_data['labels'].to(device)\n","\n","        # valid_outputs = ??? # 입력 값들을 할당하여 감정을 예측해보세요.\n","        # valid_loss = ??? # train_outputs를 활용하여 train_loss를 계산하세요.\n","        valid_outputs = gpt_clf(valid_inputs, valid_masks)\n","        valid_loss = criterion(valid_outputs.view(-1), valid_labels.float())\n","\n","\n","        # To-do\n","        # Gradients를 0으로 초기화하세요.\n","        # Back-propagation을 통해 Gradients를 계산하세요.\n","        # 계산된 Gradients를 통해 Parameter를 업데이트하세요.\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","\n","        tot_train_loss += train_loss.item()\n","        tot_valid_loss += valid_loss.item()\n","\n","        train_correct_num += cal_correct_num(torch.sigmoid(train_outputs.view(-1)), train_labels.float())\n","        valid_correct_num += cal_correct_num(torch.sigmoid(valid_outputs.view(-1)), valid_labels.float())\n","\n","\n","        if count % 200 == 0:\n","            cnt = ((count+1) * batch_size)\n","            current_train_loss = tot_train_loss / cnt\n","            current_valid_loss = tot_valid_loss / cnt\n","\n","            train_acc = train_correct_num / cnt\n","            valid_acc = valid_correct_num / cnt\n","\n","            print(f'epoch : %5d | batch : %5d | train_loss : %.5f | valid_loss : %.5f | train_acc : %.5f | valid_acc : %.5f' %(epoch+1, batch+1, current_train_loss, current_valid_loss, train_acc, valid_acc))\n","\n","            tot_train_loss = 0.0\n","            tot_valid_loss = 0.0\n","\n","            train_correct_num = 0\n","            valid_correct_num = 0\n","\n","            count = 0\n","\n","            # 이전 test_loss 보다 현재의 test_loss가 더 낮을 경우, 모델을 저장합니다.\n","            if prev_valid_loss > current_valid_loss:\n","                prev_valid_loss = current_valid_loss\n","                torch.save(gpt_clf.state_dict(), f'./KoGPT-Classifier-model.pth')\n","\n","        count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEkACEsQpA0S"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YozoT5EhpA0S"},"source":[],"execution_count":null,"outputs":[]}]}
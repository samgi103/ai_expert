{"cells":[{"cell_type":"markdown","metadata":{"id":"OFoqTA-WPoz6"},"source":["If you run in jupyter, turn\n","\n","```\n","colab = False\n","```\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"apg1TJg_ODA1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687143398305,"user_tz":-540,"elapsed":20559,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"07e1808c-225c-4abc-bc8e-9ddc2f840f83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.8.0)\n"]}],"source":["colab = True\n","if colab:\n","    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n","    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","    !apt-get update > /dev/null 2>&1\n","    !apt-get install cmake > /dev/null 2>&1\n","    !pip install --upgrade setuptools 2>&1\n","    !pip install ez_setup > /dev/null 2>&1"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"r2b4pUKfNSPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687143436277,"user_tz":-540,"elapsed":28921,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"1fc8bd48-3a5c-4d3b-b39b-2bcf1b99f0d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/양인순 교수님/6월19일실습자료/day2_q_learning\n","assets\t\t\t      optimum.npy\t     schedule.py\n","chap3_tabular_q_learning.pdf  optimum.png\t     table.npy\n","compare.py\t\t      plot.py\t\t     table.py\n","discrete_pendulum.py\t      __pycache__\t     test.py\n","learned_value.png\t      q_learning_full.ipynb  trajectory.png\n","main.py\t\t\t      q_learning.ipynb\t     video\n"]}],"source":["if colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    %cd /content/drive/MyDrive/Colab Notebooks/양인순 교수님/6월19일실습자료/day2_q_learning\n","    !ls"]},{"cell_type":"markdown","metadata":{"id":"JZ-kDzNINOPh"},"source":["# Tabular Q-learning Practice"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EQbv3yRcNOPj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687143645330,"user_tz":-540,"elapsed":451,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"dd5f5b42-b55b-498e-93fe-d9c6a3bb3227"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment DiscretePendulum-v1\u001b[0m\n","  logger.warn(f\"Overriding environment {spec.id}\")\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from gym.envs.registration import register\n","from discrete_pendulum import DiscretePendulumEnv\n","register(\n","    id=\"DiscretePendulum-v1\",\n","    entry_point=\"discrete_pendulum:DiscretePendulumEnv\",\n","    max_episode_steps=200,\n",")"]},{"cell_type":"markdown","metadata":{"id":"6CgkfEVcNOPk"},"source":["# 0. Basic definitions"]},{"cell_type":"markdown","metadata":{"id":"dm52tSrONOPk"},"source":["## 0.0. Q-table"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3gHbbASFNOPk","executionInfo":{"status":"ok","timestamp":1687143657878,"user_tz":-540,"elapsed":515,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class QTable:\n","    def __init__(self, num_states, num_actions, gamma=0.99, pth=None):\n","        self.gamma = gamma\n","        if pth is None:\n","            self.Q = -300. * np.ones(shape=(num_states, num_actions))\n","        else:\n","            self.Q = np.load(pth, allow_pickle=True)\n","\n","    def update(self, state, action, reward, next_state, alpha):\n","        # update Q-table according to the following update rule:\n","        # Q(s, a) <- Q(s, a) + alpha * (r + gamma * max_a' {Q(s', a')} - Q(s, a))\n","        # TODO_1 : Implement update target of Q-function, r + gamma * max_a' {Q(s', a')} - Q(s, a).\n","        # target =\n","        target = reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state, action]\n","        # TODO_2 : Build incremental update of Q-function.\n","        self.Q[state, action] += alpha * target\n","\n","\n","    def act(self, state):\n","        # TODO_3 : Using Q-table, choose a greedy action.\n","        #return\n","        return np.argmax(self.Q[state])\n","\n","    def save(self, pth=None):\n","        if pth is None:\n","            pth = './table.npy'\n","        np.save(pth, self.Q)\n","\n","    @property\n","    def value_ftn(self):\n","        return np.max(self.Q, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"loCG8x6RNOPl"},"source":["## 0.1. Stepsize rule & Exploration Schedule"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"BSgydQcENOPl","executionInfo":{"status":"ok","timestamp":1687144215795,"user_tz":-540,"elapsed":650,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["class LinearExplorationSchedule:\n","    def __init__(self, rollout_len, initial_epsilon=1., final_epsilon=0.02):\n","        # linear exploration schedule\n","        self.decrement = (initial_epsilon - final_epsilon) / rollout_len\n","        self.initial_epsilon = initial_epsilon\n","        self.final_epsilon = final_epsilon\n","\n","    def __call__(self, t):\n","        # Define this as callable object so that the schedule is stateless.\n","        return max(self.initial_epsilon - t * self.decrement, self.final_epsilon)\n","\n","# TODO_optional : try exponentially decaying schedule!\n","class ExponentialExplorationSchedule:\n","    def __init__(self, decay_rate, initial_epsilon=1., final_epsilon=0.02):\n","        self.decay_rate = decay_rate\n","        self.initial_epsilon = initial_epsilon\n","        self.final_epsilon = final_epsilon\n","\n","    def __call__(self, t):\n","        epsilon = self.decay_rate ** t * self.initial_epsilon\n","        return max(epsilon, self.final_epsilon)"]},{"cell_type":"markdown","metadata":{"id":"Ag8alOj2NOPm"},"source":["# 1. Q-learning"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"97_DojYsNOPm","executionInfo":{"status":"ok","timestamp":1687144341014,"user_tz":-540,"elapsed":467,"user":{"displayName":"백삼기","userId":"07015587273030351036"}}},"outputs":[],"source":["env = DiscretePendulumEnv()\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","gamma = 0.99\n","\n","learner = QTable(num_states=env.observation_space.n, num_actions=env.action_space.n, gamma=gamma)\n","rollout_len = 4000000\n","\n","\n","epsilon_schedule = LinearExplorationSchedule(rollout_len, final_epsilon=0.5)\n","# TODO_optional : alternative schedule\n","# epsilon_schedule = ExponentialExplorationSchedule(decay_rate=0.99, final_epsilon=0.02)\n","\n","checkpoint_interval = rollout_len // 20"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"OgmnuF0dNOPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687145919149,"user_tz":-540,"elapsed":1258119,"user":{"displayName":"백삼기","userId":"07015587273030351036"}},"outputId":"fd547613-80a0-4680-9873-8e718fdf3761"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","100%|██████████| 4000001/4000001 [20:57<00:00, 3179.81it/s]\n"]}],"source":["class VisitCountStepsizeSchedule:\n","    def __init__(self, deg=1.0):\n","        # polynomial stepsize schedule : $\\Theta(N_t(s, a)^{-d})$\n","        # $N_t(s, a)$ is the number of visits of (s, a)-pair until step t\n","        # to satisfy Robbins-Monro condition, d must satisfy $d \\in (1/2, 1]$\n","        assert .5 < deg <= 1\n","        self.deg = deg\n","\n","    def __call__(self, n):\n","        return 1. / ((n + 1.) ** self.deg)\n","\n","visit_count = np.zeros(shape=(num_states, num_actions))     # save visit counts N(s, a) of all state-action pairs\n","alpha_schedule = VisitCountStepsizeSchedule(deg=0.5001)\n","\n","s = env.reset()\n","for t in tqdm(range(rollout_len + 1)):\n","    u = np.random.rand()    # sampling from uniform distribution [0, 1)\n","    # TODO_4 : Implement epsilon-greedy algorithm (random action with probability epsilon, greedy action with probability 1 - epsilon).\n","    # if :\n","    #     a =\n","    # else:\n","    #     a =\n","    if u < epsilon_schedule(t) :\n","        a = env.action_space.sample()\n","    else:\n","        a = learner.act(state=s)\n","\n","    # TODO_5 : Using the chosen action, get the next state and reward.\n","    s_next, r, _, _ = env.step(action=a)\n","\n","\n","    n = visit_count[s, a]\n","    # TODO_6 : Update Q-table with QTable.update().\n","    learner.update(state=s, action=a, reward=r, next_state=s_next, alpha=alpha_schedule(n))\n","\n","    visit_count[s, a] += 1\n","    s = s_next\n","\n","    if t % checkpoint_interval == 0:\n","        learner.save()"]},{"cell_type":"markdown","metadata":{"id":"wDdUvKVlNOPo"},"source":["# 2. Let's see if the Q-function is learned properly!"]},{"cell_type":"markdown","metadata":{"id":"0E-fnx9bNOPo"},"source":["## 2.0. Visualization"]},{"cell_type":"code","source":["import gym\n","from gym.wrappers.record_video import RecordVideo\n","import os\n","from IPython.display import HTML\n","from base64 import b64encode"],"metadata":{"id":"i8QSBlhLrvG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Install classical control environment\n","!pip install gym[classic_control]"],"metadata":{"id":"Mq2o9MKRCIL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpdGqLaFNOPp"},"outputs":[],"source":["ep_len = 400\n","# test learned result!\n","trajectory = np.zeros((ep_len, 2))      # store continuous states\n","reward = 0.\n","\n","np.random.seed(2023)\n","\n","# Make directory of video record.\n","os.makedirs('./video',exist_ok=True)\n","\n","# TODO_7 : Make gym environment with id 'DiscretePendulum-v1'.\n","# env =\n","env = gym.make('DiscretePendulum-v1')\n","env = RecordVideo(env=env, video_folder='./video', name_prefix='pendulum_trained_control')\n","\n","s = env.reset(deterministic=True)\n","\n","env.start_video_recorder()\n","for t in range(ep_len):\n","    trajectory[t] = np.copy(env.x)\n","    # TODO_8 : Execute only greedy action at test time.\n","    #a =\n","    a = learner.act(s)\n","    s, r, _, _ = env.step(a)\n","    # TODO_9 : Compute cumulative reward = sum_t {gamma^t * r}.\n","    reward += (gamma ** t) * r\n","\n","print('total reward =', reward)\n","\n","# Close environment and video record.\n","env.close_video_recorder()\n","\n","mp4 = open('./video/pendulum_trained_control-episode-0.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]},{"cell_type":"markdown","metadata":{"id":"Vro4h79fNOPp"},"source":["## 2.1. Trajectory Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kltxc7mZNOPp","scrolled":true},"outputs":[],"source":["fig, ax = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n","x = env.dt * np.arange(ep_len)\n","ylabels = [r'$\\theta$ (rad)', r'$\\dot\\theta$ (rad/s)']\n","ax[1].set_xlabel(r'$t$ (s)', fontsize=20)\n","ax[0].set_ylim(-np.pi, np.pi)\n","ax[1].set_ylim(-8., 8.)\n","ax[0].set_yticks([-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n","ax[0].set_yticklabels([r'$-\\pi$', r'$-\\pi/2$', r'$0$', r'$\\pi/2$', r'$\\pi$'])\n","ax[1].set_yticks([-8, -4, 0, 4, 8])\n","for i in range(2):\n","    ax[i].plot(x, trajectory[:, i])\n","    ax[i].set_xlim(0, x[-1])\n","    ax[i].grid(True)\n","    ax[i].set_ylabel(ylabels[i], fontsize=20)\n","    ax[i].tick_params(axis='both', which='major', labelsize=18)\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"iHpvCABNNOPq"},"source":["# 2.2. Q-learning Result vs Optimal Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgBIGzVfNOPq"},"outputs":[],"source":["optimal_agent = QTable(num_states=env.observation_space.n, num_actions=env.action_space.n, pth='./optimum.npy')\n","\n","env = gym.make('DiscretePendulum-v1')\n","env = RecordVideo(env=env, video_folder='./video', name_prefix='pendulum_optimal_control')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_cetZFcNOPq"},"outputs":[],"source":["ep_len = 400\n","# test learned result!\n","trajectory = np.zeros((ep_len, 2))      # store continuous states\n","reward = 0.\n","\n","s = env.reset(deterministic=True)\n","env.start_video_recorder()\n","for t in range(ep_len):\n","    trajectory[t] = np.copy(env.x)\n","    a = optimal_agent.act(s)\n","    s, r, _, _ = env.step(a)\n","    reward += (gamma ** t) * r\n","\n","print('total reward =', reward)\n","env.close_video_recorder()\n","\n","mp4 = open('./video/pendulum_optimal_control-episode-0.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}